---
title: "CASYS :: NEXUS"
date: 2021-06-04 16:51:00 +0900
categories: Study
tags: 논문
---

Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis

## Introduction

Problem : 그래픽 작업 등에서 아주 큰 데이터셋에 대해 DNN이 자주 사용되며, GPU cluster를 어떻게 잘 utilize (workload를 GPU에 나눔) 할지가 중요해진다.

위 Problem 풀 때 고려할 점
- 한 GPU에 여러 종류의 network가 들어가며, throughput 최대화하기 위한 스케줄링 필요
- 어플리케이션이 서로 feed하는 여러 group의 DNN을 가지는 경우, 각 그룹을 명시하고 최대 성능 내도록 스케줄링 필요
- DNN을 효율적으로 계산하기 위해 batching을 쓰는데, batching은 아래와 같은 이유로 스케줄링 어렵게 함
  - batching은 cross-tenant and cross-request coordination을 필요로 함
  - binpacking-based scheduling algorithm이 batch size 포함하고 있어야 함 (batch size를 알고 있어야 하는 듯)
- 점점 더 자주 쓰이는 transfer learning은 두 task가 서로 다른 (비슷하긴 함) network를 사용하는데, 이러면 batching을 쓰기 힘듦

NEXUS
- batching-aware scheduler (Section 6.1)
- group of related DNN invocation을 쿼리로 쓰며, 최적의 batch size 제공 (section 6.2)
- 기존과 다르게 서로 다른 bathc size를 갖고 network의 일부를 batching 가능
  
latency SLO(Service Level Objective)란 이 정도보다는 좋아야 한다고 정해놓은 latency 기준인듯

## Background

vision-based applicationd에서는 stream의 frame에 DNN query를 적용하는 것이 computation bottleneck임

## 2.1 Accelerators and the challenge of utilizing them

DNN을 효율적으로 수행하는 방법은 GPU/TPU 같은 accelerator -> 많은 양의 linear algebra computation에 적합함

GPU는 CPU보다 cost도 저렴하지만, 이를 위해서는 GPU를 fully utilize해야함 -> 어마어마한 속도로 input을 넣어줘야 함.

또한 high utilization에 더해, 적절한 곳에 적절한 타입의 work를 그룹지어두는 것이 중요함

## 2.2 Placing, packing and batching DNNs

DNN은 많은 양의 계산(matrix multiplication, convolution) + 큰 dataset으로 이루어졌으며, GPU utilization에 아래와 같은 영향을 줌
- 모델을 메모리로 옮기는 것도 시간이 오래 걸리기 때문에, 보통 DNN을 GPU에 미리 옮겨두고 여러 invocation에서 사용함. 이 때 모델들을 GPU에 어떻게 co-locate하고, 어떻게 스케줄링해야 효율적일지 정해야 함
- batching을 사용하면 memory access때문에 stall할 일이 적어서 효율적인데, batch size에 따라 그 정도가 다름. 하지만 batching은 resource allocation과 scheduling을 까다롭게 만듦.

또한 batching은 같은 모델이 여러 input에 의해 사용될 때 좋은데, 요즘은 (같지 않은) smaller specialized model(ex.transfer learning)이 사용되어서 batching에 적합하지 않음. 이를 어떻게 해결할 지도 뒤에서 이야기할듯

## 3 Related works

기존의 work과 NEXUS의 차이점
- scale : 큰 workload
- expressivity : 사용자가 latency SLO 정하면 그에 맞게 batch size 등 정해 최적화 시킴 + DNN query
- granularity : 조금 다른 모델에도 적용 가능



## 4


