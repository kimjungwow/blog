---
title: "CASYS :: Stealing"
date: 2021-07-08 17:51:00 +0900
categories: Study
tags: 논문
---

# Stealing Machine Learning Models via Prediction APIs (Usenix Security 2016)

## 1 Introduction

P1 : ML은 주어진 input/output을 이용해서, 미래의 input에 대해 적절한 output을 예측하는 predictive model을 training하는 것이다. 이 때 output은 categorical or real-valued output이다.

P2 : ML 알고리즘이 좋은 성과를 내서, 점점 ML이 많이 사용되고 있다.

P3 : 클라우드 ML 환경에서 모델의 소유자는 다른 사람들이 모델에 query를 날리게 허락하기도 하는데 (돈을 받고), 모델과 training data는 잘 숨겨야 한다.
예를 들어 spam or fraud(사기) detection에서 모델이 드러나면, 그것을 아는 적들은 detection을 피할 수 있게 된다.

**P4** : 이 논문에서 다루는 model extraction model의 **threat model** : 적은 ML model에 쿼리를 보내 input feature vector에 대한 prediction을 얻을 수 있다. ML model은 black box와 같아, 적들은 model type (logistic regression, decision tree, etc.)를 알 수도 있고 모를 수도 있다. 적들의 목표는, 특정 input space에 대해 기존의 모델과 거의 동일하게 예측하는 모델을 얻어내는 것이다.

P5 : model extraction attack을 decision tree, logistic regressions, SVM, DNN 등 다양한 모델에 대해서 해봤고, Amazon과 BigML을 포함한 ML-as-a-service (MLaaS) providers들에 대해 실험했다. 타겟 모델에 가깝게 예측 가능했으며, attacker가 parameter, feature, model type를 모르는 경우에도 사용가능한 리버스 엔지니어링 등의 방법도 다룰 것이다.

P6 : 이 논문의 model extraction attack은 ML prediction APIP가 리턴하는 값에 class label 뿐 아니라 high-precision condidence value 같은 많은 정보가 담겨 있어 가능했다. class label만 리턴받는다고 가정한 이전 연구와는 다르다. 예를 들면 confidence value가 1/(1 + e ^ (-(wx+b))) 처럼 주어질 때 (x는 d차원), d+1개의 임의의 d차원 input으로 쿼리를 보내면 w와 b를 알 수 있게 된다.

P7 : P6의 방법은 logistic regression이나 NN에는 통하지만, decision tree에는 통하지 않는다. decision tree에서 confidence value는 tree의 input path에서 올바르게 labeled 된 training data의 개수이다. 어떻게 confidence value를 이용해 tree structure를 알아내는지도 다룰 것이다.

**P8** : 이 논문의 evaluation 방법을 설명함. data set은 public data를 사용, 필요한 query 수는 적음, target service의 prediction accuracy에 따라 attack에 소요되는 시간이 다름.

P9 : countermeasures = 이 논문에서 제안하는 다른 방법은 equation-solving attack보다 100배 이상의 쿼리를 필요로 함 

**P10** : contributions
- **Simple equation-solving model extraction attacks**
  - non-adaptive, random query 사용
  - confidence value가 output에 포함되어 있어야 함
- **A new path-finding algorithm for extracting decision trees** 
  - confidence value를 quasi-identifier로 이용해 path 파악함
- **Model extraction attacks against models that output only class labels**
  - confidence value 없어도 됨

P11 : 추가적으로 다루는 이야기들

## 2 Background

P1 : 이 논문에서는 ML Model이 X(input)에서 Y(output)을 얻어내는 함수로 생각함

P2 : categorical feature는 finite set of values, continuous features는 value in bounded subset of the real numbers

P3 : feature extraction하기 전에 input은 주로 preprocess되는데, preprocessing function은 주로 many-to-one임 (서로 다른 값이 같은 값으로 preprocess되는)

P4 : 이 논문은 classification에 집중한다. 단순히 class만 리턴하는 경우 뿐 아니라, input이 각 label일 확률들을 나타내는 confidence value도 의미있을 수 있다. confidence value들 중 가장 값이 큰 label이 the most probable label임

P5 : predicted output과 관련있는 개념인 distacne measure를 소개함. 특히 이 논문은 0-1 distance를 distance measure로 사용하는데, output으로 label만 주어지는 경우에는, predicted label이 실제와 같을 때만 distance가 0이고 나머지의 경우는 distance가 1이다. output으로 confidence value가 주어지는 경우에는, predicted confidence value와 실제 value의 차이들을 합한 값을 2로 나눈 값이 distance가 된다.

P6 : **Training algorithms** 이 논문에서는 supervised learning에서 얻은 모델에 집중함. 주어진 input (X,Y)들로부터 얻어낸 모델은 **model specific parameter**과 **hyper-parameter which specify type of model**에 의해 정의된다.
- hyperparameter는 주로 사람이 직접 설정하고, parameter는 학습을 통해 얻어지는 값

## 3 Model Extraction Attacks

P1 : ML model extraction attack은, **attacker가 target model f에 대한 black-box access가 가능**하고, f와 똑같거나 f에 근접하는 model을 얻으려 할 때 발생한다.

P2 : 이 논문에서 query의 output으로 label만 주어지는 경우도 다루며, 전례 없이 실용적인 방법을 제시하기도 한다. 하지만 메인은 역시 output으로 label 뿐 아니라 confidential value도 제공하고 partial feature vector도 input으로 수용하는 경우이다.

P3 : **Machine learning services** = cloud-based ML service들의 공통점은, 유저가 data를 올리면 training algorithm을 cloud에서 실행하고, query에 답할 수 있는 resulting model을 만드는 것이다.

P4 : 유저가 다운로드 받아 로컬에서 사용가능하면 white-box model이고, prediction qeury interface로만 접근 가능하다면 black-box model이다. Amazon과 Google에서는 black-box model를 제공한다.

P5 : cloud ML service 사용 방법 유사함 = 데이터 올리고 모델 선택하고 파라미터 지정하기

P6 : black-box model에서는 유저에게 input feature, input type, model class, training parameters, training data statistics처럼 부가적인 정보를 주기도 한다.

P7 : 이 논문에서는 multiple input queries도 가능하고, 일부 input feature가 없는 incomplete query도 가능하다고 가정

P8 : Google과 BigML은 model owner가 다른 유저들이 query를 통해 predict 할 수 있게 해주며 돈을 받을 수 있께 해 놓았음

P9 : **Attack scenarios** 얘기할 것임

P10 : **Avoiding query charges** : cross-user model extraction attack으로 ML model을 무료로 훔치거나, query 비용이 training/extracting 보다 비싼 점 등을 이용함

P11 : **Violating training-data privacy** : model을 통해 중요한 input data들도 노출될 수 있음

P12 : **Stepping stone to evasion** : 스팸 식별이나 malware classification처럼 ML model이 적을 감지하는데 사용되는 경우, model extraction을 통해 evasion (회피) attack이 가능함

P13 : ML security에서 중요한 가정이 있는데 (아직 말 안함), 이 논문에서는 그 가정을 깰 수 있음

### Threat model in detail

P14 : **Threat model in detail**. attacker는 direct(x를 보내면 f(x)를 얻음) query 혹은 indirect query (input space M에 대해 f(ex(M))를 얻으며, feature extraction mechanism인 ex는 attacker가 모름) 가능. 

P15 : adversary A는 최소 query로 ML model f에 근접한 모델 얻기를 목표로 한다. 이 때 ML model f에 근접하다는 것을 판단하는 기준 2가지는 다음과 같다.
- Test error R_test : f^가 f에 근접한 정도는, 모든 input x에 대해 f(x)와 f^(x)의 차이의 평균이다. 작을 수록 f^가 f에 근접한 것이다.
- Uniform error R_unif : a set U of vectors uniformly in X를 이용해, U에 속하는 모든 x에 대해 f(x)와 f^(x) 차이의 평균이다. 즉 full feature space 중 일부만 신경쓰며, U의 크기가 10000이면 충분히 크다.

P16 : extraction **accuracy**는 1에서 test/uniform error를 뺀 값으로 계산하며, distance는 앞에서 얘기한 0-1 distance를 사용한다.

P17 : attacker는 target ML model f에 대해 부가적인 정보 (f 만들때 사용된 training algorithm, hyper parameter, feature extraction function 등)를 알 수도 있다고 가정한다.
input data에 대한 정보는 ML API가 제공하는 정보로만 알고 있다고 가정한다.
즉 f가 특정 model class에 속한다고 attacker가 믿으면, 같은 class에 속하는 f^를 얻으려 노력한다.

## 4 Extraction with Confidence Values

### 4.1 Equation Solving Attacks

P1 : input x 에 대해 ML model f가 제공하는 class probability f(x)는 x에 대한 방정식으로 볼 수 있다.
이 방정식을 풀어 f에 대한 정보를 얻을 수 있다.

P2 : 이 논문에서는 input data로 public data를 사용함

P3 : preprocessing
- missing value 지움
- one-hot-encoding (서로 다른 각 값에 대한 column을 만든뒤, 0 혹은 1을 채우는 것)
- -1에서 1 사이에 속하게 scale out
데이터의 70%로 training, 나머지로 evaluation (R_test 계산)

#### 4.1.1 Binary logistic regression

P1 : binary classifier인 logistic regression이 가장 단순한 경우이다.

P2 : Logistic regression은 feature space X에서 두 개의 class를 나누는 hyperplane을 구하는 linear classifier이다. 즉 weight와 bias로 정의된다.

P3 : sample (x, f(x))에 대해, w·x+β = σ^−1(f1(x)) 와 같은 linear equation을 얻는다. 
x의 차원인 d에 대해 d+1 개의 sample을 사용해야 w와 b를 구할 수 있다.
이 때 d+1 개의 input들을 non-adaptively하게 구해지기 때문에 한 번의 batch request로 얻어질 수 있다.

P4 : 이 attack은 직관적이지만 아주 효과적이며, data set D의 크기에 상관 없이 차원 d에 대해 d+1개의 input만 있으면 가능해 효율적이다.

P5 : 이 논문에서 사용한 데이터셋에 대해, d+1 개의 input만으로도 모두 error가 0이다.

#### 4.1.2 Multiclass LRs and Multilayer Perceptrons

P1 : equation-solving attack이 logistic layer를 포함한 multiclass(c>2) LR이나 DNN에서도 잘 통함을 보일 것이다.

P2 : multiclass logistic regression(MLR)은 c개의 binary model로 이루어졌으며, 각 binary model은 각자의 weight, bias를 가진다.
MLR의 두 종류 (train/combine 방식 다름)
- softmax : 모든 training sample에 fit하는 joint multinomial distribution으로 c개의 binary model 구함
- one-vs-rest (OvR) : 각 class 별로 binary LR을 train한뒤, class probability로 normalize함

P3 : 


