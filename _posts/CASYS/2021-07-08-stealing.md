---
title: "CASYS :: Stealing"
date: 2021-07-08 17:51:00 +0900
categories: Study
tags: 논문
---

# Stealing Machine Learning Models via Prediction APIs (Usenix Security 2016)

## Context

머신 러닝이 널리 쓰임에 따라, 머신러닝 모델을 보호하는 것이 중요해졌다.

## Gap

요즘 머신 러닝을 cloud에서 돌리는 경우가 많은데, data나 model은 public한 cloud에 있으며 동시에 confidentiality를 챙기기가 어렵다.

## Innovation

MLaaS의 prediction API를 이용해 ML model을 훔치는 방법을 제안했다.
- equation solving : d+1개의 input과 그에 대한 query response를 이용해 weight과 bias를 알아낸다.
- decision tree path finding : 특정 노드를 split하는 decision boundary를 찾기 위해 feature의 값들을 조금씩 바꾸며 쿼리를 보낸다.






## 1 Introduction

P1 : ML은 주어진 input/output을 이용해서, 미래의 input에 대해 적절한 output을 예측하는 predictive model을 training하는 것이다. 이 때 output은 categorical or real-valued output이다.

P2 : ML 알고리즘이 좋은 성과를 내서, 점점 ML이 많이 사용되고 있다.

P3 : 클라우드 ML 환경에서 모델의 소유자는 다른 사람들이 모델에 query를 날리게 허락하기도 하는데 (돈을 받고), 모델과 training data는 잘 숨겨야 한다.
예를 들어 spam or fraud(사기) detection에서 모델이 드러나면, 그것을 아는 적들은 detection을 피할 수 있게 된다.

**P4** : 이 논문에서 다루는 model extraction model의 **threat model** : 적은 ML model에 쿼리를 보내 input feature vector에 대한 prediction을 얻을 수 있다. ML model은 black box와 같아, 적들은 model type (logistic regression, decision tree, etc.)를 알 수도 있고 모를 수도 있다. 적들의 목표는, 특정 input space에 대해 기존의 모델과 거의 동일하게 예측하는 모델을 얻어내는 것이다.

P5 : model extraction attack을 decision tree, logistic regressions, SVM, DNN 등 다양한 모델에 대해서 해봤고, Amazon과 BigML을 포함한 ML-as-a-service (MLaaS) providers들에 대해 실험했다. 타겟 모델에 가깝게 예측 가능했으며, attacker가 parameter, feature, model type를 모르는 경우에도 사용가능한 리버스 엔지니어링 등의 방법도 다룰 것이다.

P6 : 이 논문의 model extraction attack은 ML prediction API가 리턴하는 값에 class label 뿐 아니라 high-precision condidence value 같은 많은 정보가 담겨 있어 가능했다. class label만 리턴받는다고 가정한 이전 연구와는 다르다. 예를 들면 confidence value가 1/(1 + e ^ (-(wx+b))) 처럼 주어질 때 (x는 d차원), d+1개의 임의의 d차원 input으로 쿼리를 보내면 w와 b를 알 수 있게 된다.

P7 : P6의 방법은 logistic regression이나 NN에는 통하지만, decision tree에는 통하지 않는다. decision tree에서 confidence value는 tree의 input path에서 올바르게 labeled 된 training data의 개수이다. 어떻게 confidence value를 이용해 tree structure를 알아내는지도 다룰 것이다.

**P8** : 이 논문의 evaluation 방법을 설명함. data set은 public data를 사용, 필요한 query 수는 적음, target service의 prediction accuracy에 따라 attack에 소요되는 시간이 다름.

P9 : countermeasures = 이 논문에서 제안하는 다른 방법은 equation-solving attack보다 100배 이상의 쿼리를 필요로 함 

**P10** : contributions
- **Simple equation-solving model extraction attacks**
  - non-adaptive, random query 사용
  - confidence value가 output에 포함되어 있어야 함
- **A new path-finding algorithm for extracting decision trees** 
  - confidence value를 quasi-identifier로 이용해 path 파악함
- **Model extraction attacks against models that output only class labels**
  - confidence value 없어도 됨

P11 : 추가적으로 다루는 이야기들

## 2 Background

P1 : 이 논문에서는 ML Model이 X(input)에서 Y(output)을 얻어내는 함수로 생각함

P2 : categorical feature는 finite set of values, continuous features는 value in bounded subset of the real numbers

P3 : feature extraction하기 전에 input은 주로 preprocess되는데, preprocessing function은 주로 many-to-one임 (서로 다른 값이 같은 값으로 preprocess되는)

P4 : 이 논문은 classification에 집중한다. 단순히 class만 리턴하는 경우 뿐 아니라, input이 각 label일 확률들을 나타내는 confidence value도 의미있을 수 있다. confidence value들 중 가장 값이 큰 label이 the most probable label임

P5 : predicted output과 관련있는 개념인 distacne measure를 소개함. 특히 이 논문은 0-1 distance를 distance measure로 사용하는데, output으로 label만 주어지는 경우에는, predicted label이 실제와 같을 때만 distance가 0이고 나머지의 경우는 distance가 1이다. output으로 confidence value가 주어지는 경우에는, predicted confidence value와 실제 value의 차이들을 합한 값을 2로 나눈 값이 distance가 된다.

P6 : **Training algorithms** 이 논문에서는 supervised learning에서 얻은 모델에 집중함. 주어진 input (X,Y)들로부터 얻어낸 모델은 **model specific parameter**과 **hyper-parameter which specify type of model**에 의해 정의된다.
- hyperparameter는 주로 사람이 직접 설정하고, parameter는 학습을 통해 얻어지는 값

## 3 Model Extraction Attacks

P1 : ML model extraction attack은, **attacker가 target model f에 대한 black-box access가 가능**하고, f와 똑같거나 f에 근접하는 model을 얻으려 할 때 발생한다.

P2 : 이 논문에서 query의 output으로 label만 주어지는 경우도 다루며, 전례 없이 실용적인 방법을 제시하기도 한다. 하지만 메인은 역시 output으로 label 뿐 아니라 confidential value도 제공하고 partial feature vector도 input으로 수용하는 경우이다.

P3 : **Machine learning services** = cloud-based ML service들의 공통점은, 유저가 data를 올리면 training algorithm을 cloud에서 실행하고, query에 답할 수 있는 resulting model을 만드는 것이다.

P4 : 유저가 다운로드 받아 로컬에서 사용가능하면 white-box model이고, prediction qeury interface로만 접근 가능하다면 black-box model이다. Amazon과 Google에서는 black-box model를 제공한다.

P5 : cloud ML service 사용 방법 유사함 = 데이터 올리고 모델 선택하고 파라미터 지정하기

P6 : black-box model에서는 유저에게 input feature, input type, model class, training parameters, training data statistics처럼 부가적인 정보를 주기도 한다.

P7 : 이 논문에서는 multiple input queries도 가능하고, 일부 input feature가 없는 incomplete query도 가능하다고 가정

P8 : Google과 BigML은 model owner가 다른 유저들이 query를 통해 predict 할 수 있게 해주며 돈을 받을 수 있게 해 놓았음

P9 : **Attack scenarios** 얘기할 것임

P10 : **Avoiding query charges** : cross-user model extraction attack으로 ML model을 무료로 훔치거나, query 비용이 training/extracting 보다 비싼 점 등을 이용함

P11 : **Violating training-data privacy** : model을 통해 중요한 input data들도 노출될 수 있음

P12 : **Stepping stone to evasion** : 스팸 식별이나 malware classification처럼 ML model이 적을 감지하는데 사용되는 경우, model extraction을 통해 evasion (회피) attack이 가능함

P13 : ML security에서 중요한 가정이 있는데 (아직 말 안함), 이 논문에서는 그 가정을 깰 수 있음

### Threat model in detail

P14 : **Threat model in detail**. attacker는 direct(x를 보내면 f(x)를 얻음) query 혹은 indirect query (input space M에 대해 f(ex(M))를 얻으며, feature extraction mechanism인 ex는 attacker가 모름) 가능. 

P15 : adversary A는 최소 query로 ML model f에 근접한 모델 얻기를 목표로 한다. 이 때 ML model f에 근접하다는 것을 판단하는 기준 2가지는 다음과 같다.
- Test error R_test : f^가 f에 근접한 정도는, 모든 input x에 대해 f(x)와 f^(x)의 차이의 평균이다. 작을 수록 f^가 f에 근접한 것이다.
- Uniform error R_unif : a set U of vectors uniformly in X를 이용해, U에 속하는 모든 x에 대해 f(x)와 f^(x) 차이의 평균이다. 즉 full feature space 중 일부만 신경쓰며, U의 크기가 10000이면 충분히 크다.

P16 : extraction **accuracy**는 1에서 test/uniform error를 뺀 값으로 계산하며, distance는 앞에서 얘기한 0-1 distance를 사용한다.

P17 : attacker는 target ML model f에 대해 부가적인 정보 (f 만들때 사용된 training algorithm, hyper parameter, feature extraction function 등)를 알 수도 있다고 가정한다.
input data에 대한 정보는 ML API가 제공하는 정보로만 알고 있다고 가정한다.
즉 f가 특정 model class에 속한다고 attacker가 믿으면, 같은 class에 속하는 f^를 얻으려 노력한다.

## 4 Extraction with Confidence Values

### 4.1 Equation Solving Attacks

P1 : input x 에 대해 ML model f가 제공하는 class probability f(x)는 x에 대한 방정식으로 볼 수 있다.
이 방정식을 풀어 f에 대한 정보를 얻을 수 있다.

P2 : 이 논문에서는 input data로 public data를 사용함

P3 : preprocessing
- missing value 지움
- one-hot-encoding (서로 다른 각 값에 대한 column을 만든뒤, 0 혹은 1을 채우는 것)
- -1에서 1 사이에 속하게 scale out
데이터의 70%로 training, 나머지로 evaluation (R_test 계산)

#### 4.1.1 Binary logistic regression

P1 : binary classifier인 logistic regression이 가장 단순한 경우이다.

P2 : Logistic regression은 feature space X에서 두 개의 class를 나누는 hyperplane을 구하는 linear classifier이다. 즉 weight와 bias로 정의된다.

P3 : sample (x, f(x))에 대해, w·x+β = σ^−1(f1(x)) 와 같은 linear equation을 얻는다. 
x의 차원인 d에 대해 d+1 개의 sample을 사용해야 w와 b를 구할 수 있다.
이 때 d+1 개의 input들을 non-adaptively하게 구해지기 때문에 한 번의 batch request로 얻어질 수 있다.

P4 : 이 attack은 직관적이지만 아주 효과적이며, data set D의 크기에 상관 없이 차원 d에 대해 d+1개의 input만 있으면 가능해 효율적이다.

P5 : 이 논문에서 사용한 데이터셋에 대해, d+1 개의 input만으로도 모두 error가 0이다.

#### 4.1.2 Multiclass LRs and Multilayer Perceptrons

P1 : equation-solving attack이 logistic layer를 포함한 multiclass(c>2) LR이나 DNN에서도 잘 통함을 보일 것이다.

P2 : multiclass logistic regression(MLR)은 c개의 binary model로 이루어졌으며, 각 binary model은 각자의 weight, bias를 가진다.
MLR의 두 종류 (train/combine 방식 다름)
- softmax : 모든 training sample에 fit하는 joint multinomial distribution으로 c개의 binary model 구함
- one-vs-rest (OvR) : 각 class 별로 binary LR을 train한뒤, class probability로 normalize함

P3 : input dimension d, the number of classes c에 대해 MLR model f는 w in R^cd 와 b in R^c 로 정의된다. 따라서 MLR의 equation은 non-linear여서, 직접 풀기는 힘들고 loss function을 최소화하는 방식으로 푼다. (loss function은 strongly convex여서 global minimum으로 수렴한다.)

P4 : multilayer perceptron (MLP)는 모든 input에 non-linear transform을 적용하는 hidden layer를 가지고 있어서, multiclass logistic regression (MLR)보다 풀기가 어렵다.
또한 loss function도 strongly convex가 아니어서, global minimum이 아닌 local minimum으로 수렴해 정확히 target ML model f에 근접하기가 어렵다.

P5 : non-linear equation은 d+1개의 input이면 충분했던 linear equation과 다르게 얼마나 많은 input이 필요한지 모른다. 따라서 unknown model parameter의 개수와 budget scaling factor를 고려해 input의 수를 정했다.

P6 : class 개수가 5일 때, Softmax와 OvR같은 MLR model은 unknown parameter 한 개 당 하나의 쿼리 정도면 충분했고, MLP에서는 4배 정도의 쿼리가 있어야 f에 근접할 수 있었다.

P7 : class 개수 c가 바뀌면, MLR은 c(d+1) 개의 input이 필요했고, MLP는 훨씬 많이 필요했다. 역시 cross-user attack으로 multiclass Logistic regression 도 공격할 수 있었다.

#### 4.1.3 Training Data Leakage for Kernel LR

P1 : kernel logistic regression 다룰 것

P2 : kernel method는 Support Vector Machine를 nonlinear classifier로 바꾸는데 주로 사용되지만, class probabilities를 나타낼 수 있어 logistic regression에도 사용될 수 있다. 하지만 아직 kernel LR을 지원하는 ML library는 없다.

P3 : KLR model은 f(x) = ∑(s,r=1) α_(i,r)K(x,x_r) +β_i 의 꼴이며, x_1, ... x_s들은 representers로, training points 중 선택된다.

P4 : KLR model이 query에 응답하며 리턴하는 equation을 풀기 위해서는 사전에 선택된 representer의 개수 s를 알아야 한다는 가정이 있다.
- 이 논문에서는 s보다 큰 s' 개의 representer를 가정해도 model f에 근접할 수 있다는 아이디어를 이용한다.
  - 추가 representer들이 있어도 α=0이면 값이 바뀌지 않음을 이용
- 또한 s보다 작은 s'개의 representers로도 f에 근접할 수 있음을 보일 것이다.

P5 : 실제로 kernel LR에 대해 model extraction attack을 실험한 예시 = the number of representers s를 아는 경우와 모르는 경우를 나눠서 실험함

#### 4.1.4 Model Inversion Attacks on Extracted Models

P1 : **model inversion attack**은 classifier f에 접근 가능함을 이용해, class i일 확률이 가장 높은 input x_opt를 찾는 것이다. 이를 통해 training set이 노출될 수 있다.

P2 : model inversion attack은 white-box에서 강력하며, black-box에서도 효과가 있다.

P3 : model extraction 후 white-box inversion attack을 통해 local computation을 진행하면 더 빠르고 쿼리도 덜 필요함

P4 : 어떤 데이터를 사용했는지 + 결과 이야기함

P5 : trained softmax model은 binary logistic regression보다 더 복잡해서 푸는데 오래 걸렸지만, white-box를 이용해 훨씬 효율적으로 공격할 수 있었다.

P6 : P5의 경우에 black-box를 이용하면 시간이 너무 오래 걸린다.

P7 : (요약) model extraction 후 model inversion을 하면 훨씬 적은 online query로도 공격 가능함

### 4.2 Decision Tree Path-Finding Attacks

P1 : Decision tree는 logistic model과 다르게 연속함수 형태의 class probability가 아닌, input space를 discrete region으로 나눈다.

P2 : decision tree extraction 관련 이전 연구는 boolean feature/output만을 가지는 decision tree만을 고려했는데, 이것은 현실적이지 않다.

P3 : 이 논문에서 고려하는 decision tree는 categorical features를 3개 이상으로 나누는 것도 가능하고, boolean feature 뿐 아니라 numeric feature도 가능하다. 각 leaf가 class label과 confidence score로 분류되는 tree뿐 아니라, leaf가 real-valued output과 confidence로 분류되는 regression tree도 이 논문은 다룰 수 있다.

p4 : query 응답이 담은 정보들로 input이 decision tree에서 지나는 path 예측함.
- 특정 feature 값을 조금씩 바꿈
- incomplete query도 사용가능하게 함

P5 : identity orcale이란 query x를 input 으로 받으면, x가 도달하는 leaf of tree의 identifier를 리턴한다

#### 4.2.1 Extraction Algorithms

![algorithm1](https://imgur.com/mIIFmfu.png)
P1 : 이 논문의 path-find attack은 algorithm1에 나오는데, 우선 leaf-identity oracle이 각 leaf의 unique identifier를 리턴한다고 가정한다.
- random input x와 그것에 대한 leaf identifer id를 oracle로 부터 얻음
- x가 해당 leaf에 남아있기 위한 조건을 판단한다.
  - LINE_SEARCH : contiguous feature
  - CAT_SPLIT : categorical feature
- 이를 통해 아직 방문되지 않은 leaves의 조건들도 파악해나간다.
- output : 알고리즘은 모든 leaf에 대해 해당 leaf에 도달하기 위한 input x의 constraint를 리턴한다.


![fig3](https://imgur.com/atlEzxm.png)
P2 : 예시와 함께 decision tree extraction algorithm 설명, 아래와 같은 2가지 목표
- 현재 leaf에 대한 input x의 constraint 찾기
- 다른 leaf로 향하는 input x' 찾기

P3 : algorithm1의 LINE_SEARCH (line 12)는 continuous feature를 테스트하는데, 처음에는 가능한 모든 범위부터 시작해서 binary search를 통해 현재 leaf에 해당하는 범위를 알아낸다.

P3 : algorithm1의 CATEGORY_SPLIT (line 20)는 categorical feature를 split하는 기준을 찾는데, 현재 leaf로 향하는 categories와 다른 leaves로 향하는 categories로 구분한다.

P4 : P2,3,4를 통해 현재 leaf에 대한 input x의 constraint와 다른 leaf로 향하는 input x'를 찾을 수 있었다.

##### A top-down approach
P5 : queries over partial inputs를 이용하는 top-down alogrithm = 모든 feature의 value가 없는 query를 이용해 root의 identifer를 얻고, feature를 조금씩 늘리며 tree를 layer씩 파악하는 방법

##### Duplicate identities
P6 : alogirhtm 1의 line 7이 current id가 이미 방문했는지 뿐 아니라, 현재 query가 해당 leaf의 predicate를 만족하는지 확인함으로써 duplicate identifier로 잘 처리할 수 있다.

#### 4.2.2 Attack Evaluation
P1 : 이 논문에서 사용한 tree model 설명, 우선 tree를 다운로드한 뒤 local에서 쿼리 날림

P2 : black-box access를 local에서 실험했는데, 아래와 같은 field in query responses를 유용하게 사용했다.
- Prediction : the predicted class label (classification) or real-valued output (regression)
- Confidence score
- Fields : input query 혹은 traversed path에 포함되는 모든 feature가 'fields' property에 담겨있다. 특히 partial query가 internal node v에 도착한다면, input x에는 없지만 fields에는 있는 feature가 node v에서 splited됨을 알 수 있어서 top-down approach에 유용하다.

P3 : 결과 분석 = duplicate identifer가 없거나 incomplete query를 사용해야 attack이 더 효율적이다.

## 5 Online Model Extraction Attacks

online attack 다룰 것

### 5.1 Case Study 1: BigML

decision tree extraction 성공, 1150번 이상 prediction하려는 사람은 이 논문처럼 attack하는 것이 더 효율적임

### 5.2 Case Study 2: Amazon Web Services

P1
- 아마존은 logistic regression으로 classfication한다. 
- 2개의 feature extraction techniques
  - one-hot-encoding
  - quantile binning (numeric features의 범위를 k개로 나눈 뒤 k개의 binary feature encoding으로 표현)
- feature extraction function ex를 reverse-engineering으로 알아내면, local에서 x=ex(M)을 계산한 후 equation-solving으로 ML model f를 알아낼 수 있음

P2 : Amazon attack 예시중 제일 단순한 경우 : categorical feature 없고, quantile binning 없는 경우에 에러는 0이었음

P3 : 다음으로 feature extraction 있는 경우
- adversary가 input space M는 알고 있어서 one-hot-encoding은 local에서 처리할 수 있음
- 하지만 training data에 대한 정보는 없어서, decision tree의 LINE_SEARCH와 비슷한 방법으로 quantile binning의 각 범위들을 알아냄
- Amazon은 query의 missing feature에 0을 채우기 때문에, feature 하나에만 값을 넣어 query를 보내면 한 개의 unknown parameter에 대한 equation을 얻을 수 있음

P4 : attack 예시 설명
- quantilie bin에 사용한 query를 equation-solving에도 사용해 최적화

### 5.3 Discussion

#### Additional feature extractors

P1 : feature extraction은 input feature가 아닌 learned weight에 적용되는 것으로 볼 수 있으므로, f(ex(M))~=f'인 f'는 un-scaled input space M에 대한 모델로 해석할 수 있다.

P2 : 이 논문에서 다루지 않은 feature extractor들 언급

#### Learning unknown model classes or hyper-parameters.

P1 : ML model class, feature extraction function, hyper-parameter 같은 것은 ML service로 부터 조금씩 범위를 좁혀가는 방식으로 알아낼 수 있다.

P2 : attacker는 동일한 extraction/test samples를 사용해, error가 충분히 작을 때까지 model extraction을 여러 번 수행한다.

P3 : Amazon attack 과정
- documentationi에서 밝혀지지 않은 model characteristic 예측
- extraction 시도하며 논문 저자들의 가정 평가
- 그를 통해 Amazon이 softmax regression을 사용하며, missing value에 대해서는 binary predictor를 만들지 않음을 알아냄
- BigML은 반대 (softmax 대신 OvR, binary predictor 만듦)


## 6 Extraction Given Class Labels Only

label만 리턴하는 경우

#### The Lowd-Meek attack

P1 : black-box linear classifier label만 리턴하는 경우를 다룬 이전 연구에서는 LINE_SEARCH를 이용해 decision boundary에 가까운 지점을 찾음

P2 : P1의 이전 연구는 linear binary model에만 적용되서, non-linear model에도 적용하는 방법을 수학적으로 제시함

#### The retraining approach

주어진 input-output examples를 이용해 local에서 model을 re-train 하는 방법도 있다. queried samples에 대해 error가 적은 모델을 구하면 그것이 실제 ML model f와 가깝기를 기대한 방법인데, 세 가지 방법이 있다.
- Retraining with uniform queries : uniformly selected points를 이용해 retrain
- Line-search retraining : line search를 이용해 decision boundary에 가까운 sample을 찾은 뒤, 그 sample들을 이용해 retrain
- Adaptive retraining : 여러 라운드에 걸쳐 uniform points를 고르고 train 한 뒤 decision boundary 근처에서 또 uniform points를 고르고 train하는 과정을 반복함

### 6.1 Linear Binary Models

P1 : Lowd-Meek attack과, 윗 문단에서 제시한 세 가지 retraining strategy를 이용해 linear binary model에 적용해봄

P2 : budget과 error를 이용해 평가하면 budget이 커지면 Lowd-Meek가 제일 효율적이고, uniform query 쓰는 것 보다는 decision boundary 근처의 point 이용하는 것이 더 효율적임

P3 : 하지만 필요한 query 수를 비교하면 Lowd-Meek보다 adaptive retraining이 훨씬 적으므로, label만 리턴하는 linear model에서도 더 효율적인 extraction이 가능할 것으로 보인다.

P4 : user가 아닌 ML service가 feature-extraction을 제공하는 경우, input space M에 대해 indirect query를 하는 것만 가능하다. Lowd-Meek attack은 indirect query가 불가능하지만, 이 논문에서 제안한 retraining strategies는 indirect query를 써도 성능에 큰 차이가 없다.

### 6.2 Multiclass LR Models

P1 : multiclass에서는 Lowd-Meek 사용할 수 없어서, 논문이 제안한 retraining 방법들만 평가하려 한다.

P2 : query 응답이 class만 포함한다면 softmax나 one-vs-rest의 output이 동일해서, 둘 중 어떤 것을 사용해도 상관 없다. 이 논문은 softmax 사용할 것이다.

P3 : 결과 분석
- adaptive가 가장 효율적
- decision boundaries가 여러 개인 경우, line_search는 uniform와 비슷한 정도임
- 항상 R_test가 R_unif보다 낮음. 대체로 test set의 점들은 model f의 decision boundary에서 멀다는 의미임

P4 : 비록 class만 사용하는 경우 MLR model extraction attack은 confidence score도 사용하는 경우보다는 100배 많은 쿼리를 필요로 하지만, 유용하게 쓰일 수 있다.

### 6.3 Neural Networks

P1 : DNN은 multiclass regression보다 parameter도 많고, non-linear decisino boundary도 가져서 class만 활용할 수 있는 경우 model extraction attack이 더 어려울 것으로 예측했었다.

P2 : 결과
- 실제로 line-search와 adaptive는 uniform retraining에 비해 나은 점이 거의 없음
- 너무 쿼리가 많이 필요해서, 정직하게 ML API로 쿼리 날리는 것이 model extraction 하는 것보다 쌀 수도 있음

### 6.4 RBF Kernel SVMs

P1 : radial-basis function(RBF) kernel을 가지는 support-vector machines (SVM)도 nonlinear model이다.
kernel SVM은 input을 더 높은 차원으로 transform하는데, RBF kernel에서는 그 차원이 무한해서 두 클래스로 나누는 hyperplane을 Lowd-Meek attack으로 찾을 수 없다.

P2 :
- SVM은 class만 리턴함 -> retrainig만 가능
- RBF kernel의 hyper-parameter를 알아내기 위해 extraction-and-test 기법 사용
- adaptive retraining이 제일 효율적임

## 7 Extraction Countermeasures

P1 : 이 논문에서 다룬 model extraction attack에 대한 보호 대책을 다룰 것

P2 : section 6에서 다룬 내용 재언급
- prediction API minimization (class만 리턴)을 쓰면 이 논문에서 다룬 대다수의 attack을 방지할 수 있음
- 하지만 class만 리턴해도 공격이 가능하긴 함

#### Rounding confidences
P1 : confidence는 제공하지만 정확한 값이 아닌 근사치를 제공하는 방법이다.

P2 : rounding confidence를 사용한다면
- equation solving은 정확한 model f에 접근하는 것이 아닌, 조금 다른 모델에 접근하는 것이고
- decision tree path finding attack은 node identifier가 중복될 확률이 높아져, attack 성공 확률을 낮춘다.

P3 : 결과 분석
- 소수점 4,5 째 자리로 근사시키는 것은 R_test에 큰 차이가 없다.
- 소수점 둘, 셋째 자리로 근사시키면 R_test가 증가한다.
- rounding confidence를 해도, confidence가 아닌 class label만 제공하는 경우보다는 효율적인 공격이 가능하다.

#### Differential privacy

P1 : Differential privacy (DP)가 extraction을 막을 수 있을 지를 논의할 것

P2 : DP는 individual training data element를 지키기 위한 것이지, model extraction을 막는 용도가 아니다. 예를 들어, training data를 무시하고 weight와 bias를 0으로 설정하면, training data는 노출되지 않지만 model (w와 b 값)은 쉽게 노출된다.

P3 : 차라리 training data가 아닌 model parameter에 DP를 적용해, neighboring model parameter와 구분할 수 없게 하는 것이 더 맞다. -> future work

#### Ensemble methods
- Ensemble method는 target function의 근사치를 리턴하기 때문에, model extraction attack이 치명적이지는 않을 것이다.
- 하지만 model evasion같은 다른 attack은 치명적일 수 있다.

## 8 Related work
P1 : PAC learning과 model extraction의 차이점
- ?
- 이전 연구보다 prediction API가 더 많은 정보를 준다고 이 논문은 가정함

P2 : 이전 연구
- membership queries 관련 이전 연구들이 많았음
- model extraction이 model evasion(회피) 보다 어려움

P3 : evasion attack은 adversarial machine learning과도 관련이 있다. 또한 train/test data에 악의적으로 crafted sample을 넣는 poisoning attack과도 관련이 있다.
