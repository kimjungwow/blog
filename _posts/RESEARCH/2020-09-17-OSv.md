---
title: "Research :: Osv"
date: 2020-09-17 18:22:00 +0900
categories: Study
tags: Study OS 논문
---

# OSv—Optimizing the Operating System for Virtual Machines 

## 1 Introduction

Cloud computing은 virtualization을 통해 여러 조직이 하나의 machine을 공유하기 좋아서 발달함.

기존에는 physical machine에서 사용되는 Linux, Windows, BSD 등의 OS가 VM에서 사용되었지만, 이러한 OS들이 physical machine에서 사용된 이유 (한 개의 machine을 관리하기 적절 + 다양한 HW 지원)는 VM에서는 의미가 적어졌다. 또한, VM이 빠르고 작고 관리하기 쉬워야 한다는 것이 중요해졌다. 



기존 OS의 기능이 cloud 내 여러 layer에서 중복되어 있어서 overhead가 되었다.

**duplicate 1 : isolate processes**
예를 들면 프로세스 사이의 isolation을 OS가 기존에 신경썼지만, VM에서는 hypervisor가 서로 다른 VM 사이의 isolation을 신경쓴다. 주로 하나의 VM에서 하나의 app을 실행하여 isolation을 잘해서, (기존 OS의 process isolation이 VM에서 쓸 OS는 필요가 없다.)

**duplicate 2 : HW abstraction**
기존의 OS는 app이 HW를 사용할 때 HW abstraction을 제공하지만, VM에서는 hypervisor에 의해 추상화된 HW를 app이 사용한다. 이 duplication도 성능에 영향 미친다.

**Physical machine이 아닌 VM에서 사용할 목적으로 OS를 만들면 어떨까? -> OSv**

OSv의 목표
1. Run existing cloud applications (Linux executables)
1. Run these applications faster than Linux does.
1. Make the image small enough, and the boot quick enough, that staring a new VM becomes a viable alternative to reconfiguring a running one.
1. Explore new APIs for new applications written for OSv, that provide even better performance.
1. Explore using such new APIs in common runtime environments, such as the Java Virtual Machine(JVM). This will boost the performance of unmodified Java applications running on OSv.
1. Be a platform for continued research on VM operating systems. OSv is actively developed as open source, it is written in a modern language (C++11), its codebase is relatively small, and our community encourages experimentation and innovation.

OSv는 architecture-specific code를 최소화하며 다양한 hypervisor와 processors를 지원한다.

## 2 Design and Implementation of OSv

OSv 디자인은 exokernel과 비슷하다. Hypervisor가 exokernel, VM이 application 역할과 비슷하다. 

OSv에서는 VM당 하나의 application을 가정 -> OSv 구성이 단순해지고, VM 내부에서의 isolation은 필요 없다. Hypervisor만 isolation 신경쓰면 된다.  
OSv는 **하나의 address space**를 사용한다. 커널과 모든 thread가 같은 page table을 쓰고, context switching (app thread 사이 혹은 app thread <-> kernel thread) 비용이 적어진다.

OSv의 `ELF dynamic linker` : 리눅스 ABI의 함수를 사용하면 이 linker가 OSv kernel내의 함수를 호출한다. 이 때, `read()` 등 system call도 OSv는 single-application OS이기 때문에 user-to-kernel overhead를 없앨 수 있으므로, system call이 아닌 function call처럼 동작한다.

기존의 app도 OSv에서 잘 동작하기 위해 Linux를 많이 모방했지만, single-app OS에서 필요 없는 `fork(), exec()`은 구현하지 않았다.

OSv는 다양한 HW를 지원하는 physical machine에서의 OS가 아니다. Hypervisor가 더욱 단순한 HW abstraction을 제공하기 때문에, OSv는 자주 사용되는 키보드 등의 장치들을 위한 driver만을 가진다.

### 2.1 Memory Management

libOS는 flat memory 사용하지만, OSv는 일반 OS처럼 virtual memory 사용하는데, 그 이유는
1. x86_64 아키텍쳐가 long mode(64-bit) operation을 사용해서
1. 최근 app들이 map/unmap + 스스로 page protection을 사용해서

OSv는 `mmap` 사용.  
mapping이 크면 OSv는 2MB처럼 큰 사이즈의 페이지 사용 -> TLB miss 적어져 성능 개선  
페이지의 일부만 unmap 돼야 하는 경우, 페이지를 더 작은 크기의 페이지로 나누는 기능 구현  
single-app OS여서 page-evict X

### 2.2 No Spinlocks

Single processor OS에서는 interrupt, context switch를 끄는 것으로 충분하지만, physical machine의 Multi processor OS에서는 여러 CPU가 동시에 같은 데이터를 접근할 수 있어서 spinlock을 얻은 뒤에만 수정가능하게 했다.  
계속 동작하는 Physical CPU와 달리, Virtual CPU는 중간에 멈추기도 한다. (hypervisor에 하거나, hypervisor가 다른 guest 실행하는 등의 이유로.) 이 때 lock을 쥔 상태로 CPU가 멈추면, spinlock을 사용하는 경우 그 lock을 쥐고자하는 다른 CPU는 (sleep하지 않고) spin하며 기다리기 때문에 성능이 저하된다.  
이를 lock-holder preemption problem이라 하는데, OSv는 아예 spin-lock을 사용하지 않음으로써 이를 해결한다. 하지만 그렇다고 일부 기능을 포기하거나 single-processor로 제한되지 않는다.  

OSv에서는
1. 대부분 커널에서의 작업이 thread에서 이루어지며, sleep 가능한 Mutex (Spin-lock X)를 이용한다. 이를 위해 thread context switch의 비용이 적어야 한다.
1. 이 때 Mutex는 lock-free algorithm으로 구현한다.
1. scheduler는 thread에서 동작할 수 없으므로, per-cpu run queues(대부분의 작업이 local to CPU여서 locking이 필요 없음)와 lock-free algorithm(다른 cpu의 thread를 깨우는 경우)을 사용한다.

lock-free algorithm으로 mutex를 구현하려면 `compare-exchange, fetch-and-add`같은 atomic operation을 이용한다.

### 2.3 Network Channels

cloud에서 사용될 OS는 TCP/IP stack이 좋아야 한다.  
networking stack은 top-down (Application thread)와 bottom-up(IRQ context)로 traversed되는데, 이 때 양 방향에서 shared data structure에 접근한다.  
따라서 OSv에서는 대부분의 작업을 application에서 진행함으로써 여러 개의 thread가 shared data structure에 동시에 접근할 일이 없게 한다. 패킷이 도착하면 application thread에 전달하는 channel을 만드는 방식이다.  
이처럼 application thread 한 개만 data에 접근하므로, receive용과 send용 socket buffer lock을 하나로 합칠 수 있다. 또한 interleave lock은 wait queue로 대체하고, TCP layer는 바로 위 socket layer와 함께 동작하므로 TCP layer의 lock을 없앨 수 있다.

### 2.4 The Thread Scheduler

OSv scheduler의 목표는 **lock-free, preemptive, tick-less, fair, scalable, and efficient**이다.

#### Lock-free

OSv scheduler는 CPU마다 runnable threads를 모아둔 run queue를 가진다. 각 CPU마다 queue 있어서 locking 필요 없다. running thread가 reschedule 요청하거나, timer expiration으로 preemption이 요구될 때 scheduler가 CPU에서 동작한다.  
서로 다른 CPU의 run queue에 thread 수가 다를 경우, load balancer가 많은 쪽의 thread 하나를 다른 (remote) CPU에서 wake up하는 식으로 균형을 맞춘다.  
`N`개의 CPU 각각에 총 `N`개의 lock-free queues of *incoming wakeups* 가진다. CPU s가 CPU d에서 thread wakeup 시키고 싶은 경우, queue (s,d)에 넣고 (CPU d의 것인듯), CPU d의 bitmask를 설정한 뒤, CPU d에게 inter-processor interrupt (IPI)를 보낸다. IPI로 인해 CPU d는 reschedule을 수행하는데, 이 때 incoming wakeups를 먼저 확인한다. bitmask를 통해 s의 thread가 있는 incoming queue를 확인할 수 있다.

#### Preemptive

kernel thread, application threads 모두 (wait, yield, wake up으로 ) reschedule 될 수도 있고 timer나 wakeup IPI로 preempted 될 수도 있다. per-thread preempt-disable counter를 늘림으로써 일시적으로 preemtped 되지 않을 수 있는데, 이는 per-CPU variables나 RCU lock을 유지할 떄 유용하다.

#### Tick-less

통상 kernel은 정기적으로 timer interrupt (tick)을 발생시켜 reschedule한다. 각 thread별로 실행한 시간 (ticks)를 계산하여, 어떤 thread를 schedule할지 정하기도 한다.  
하지만 과도한 tick(timer interrupt)은 CPU time을 낭비한다. VM에서는 timer interrupt가 exit to hypervisor도 포함해 더 오래 걸려서, CPU time이 더 많이 낭비된다.  
따라서 OSv는 tick-less 디자인을 선택했는데, high resolution clock을 이용함으로써 thread의 정확한 실행 시간을 계산한다. (ticks로 어림잡지 않음) thread를 실행할 때 언제 다음 thread로 switch할지를 기억할 때는 timer interrupt가 사용되며, 비슷한 priority의 두 thread가 있는 경우 너무 자주 switch 되지 않도록 한다. (hysteresis 이력 현상)

#### Fair

각 thread의 전체 실행 시간을 맞추려고 하면, 나중에 들어오는 thread가 CPU를 독점할 수 있다. 따라서 OSv는 먼 과거는 무시하고 각 스레드의 최근 일정 기간동안의 실행시간(moving-average)을 맞춘다.  
이 때 실행 시간은 floating point로 계산한다. 통상 커널에서는 floating point를 사용하지 않지만, OSv는 kernel과 applications의 차이가 명확치 않아서 floating point를 커널에서도 사용한다.  
모든 thread의 moving-average를 측정하는 것은 부담이 큰데 (scalability), **OSv는 single running thread만 moving-average를 측정한다.** (이 부분이 이해되지 않음. 각 reschedule이 하나의 running thread의 moving-average만을 update한다는 의미인듯)

#### Scalable

OSv의 scheduler는 각 CPU내 thread 개수 `N`에 대해 `O(lgN)` 복잡도 가진다. run-queue의 thread는 moving-average에 따라 정렬되는데, 이 때 running thread가 아닌 thread들은 scheduler가 신경쓰지 않아 성능 저하가 없다. interrupt-handling thread나 load-balancer 처럼 가끔씩 running하는 thread들이 있어도 성능이 저하되지 않는다.

#### Efficient

OSv의 scheduler가 효율적인 이유
1. OSv는 기존의 multi-processor OS와 달리 single address space여서 context switch시 page table을 바꾸거나 TLB를 flush 안 시켜도 되서 빠르다.  
1. Floating-point unit (FPU) 레지스터를 매 context switch시 저장하는 것도 cost가 크다. OSv에서의 대부분 reschedule은 *voluntary*여서 (`wake()`, `mutex_wait()`에 의한), FPU registers들이 caller-saved 되도록 한다. 따라서 voluntary context switch시 FPU 레지스터 저장하지 않음

다른 CPU의 thread를 깨우기 위해 IPI를 사용하는 것은 VM에서 cost가 크다. (보내고 받는 것이 모두 exit to hypervisor를 포함해서)  
이를 해결하기 위해 thread가 polling state에서 halt할 때, 잠시 IPI를 받지 않도록 설정한뒤 wakeup bitmask를 확인함으로써, 두 CPU가 하나의 lockstep을 기다리고 있는 경우 cost가 큰 IPI를 없앨 수 있다. (이 부분이 이해되지 않음)

## 3 Beyond the Linux APIs

OSv와 같은 single-application OS에서 적절한 새로운 API를 만들기 위해, `JVM` 같은 *runtime environment*에서 효율적으로 동작하는 것을 목표로 한다. (JVM을 최적화하여, JVM에서 실행되는 app이 효율적으로 실행됨)  
OSv는 single address space에서 single-application 가정하기 때문에, system call을 function call로 취급하는 방식으로 Linux API를 사용한 app들을 최적화했다.  
하지만 그런 app들은 multi-process multi-user OS를 고려했기 때문에, single-process OS인 OSv에서 overhead가 있다. 그 overhead를 해결하기 위한 방법을 여기서 다룰 것이다.  

예 1 : socket  
일반적인 socket의 read, write시 kernel은 user space와 packet buffer를 공유할 수 없으므로, data를 복사한다. 하지만 single-address-space인 OSv에서는 공유할 수 있어서, 복사할 필요가 없다.  
OSv에서는 app에게 host's virtio ring을 노출하기도 하는데, 이는 single-application이어서 안전하며, abstraction layer 하나를 없애준다.  

예 2 : app이 page table에 직접 접근하도록 하는 것  
JVM이 write access to reference to object를 기록하기 위해 card table이나 write barrier를 사용하는데, MMU에 JVM이 직접 접근하도록 함으로써 overhead를 없앤 아이디어  

OSv용 non-Linux feature 두 가지 제시
1. *shrinker* API : app과 kernel이 모든 available memory를 공유하도록 함
1. *JVM balloon* : shrinker 아이디어를 JVM에 적용하여, heap size를 일일이 지정하는 대신 kernel에 의해 사용되지 않는 메모리 크기에 자동으로 맞춰짐

### 3.1 Shrinker

Shrinker API는 application이나 OS component가 system에 메모리가 부족한 경우 OSv가 호출할 callback function을 등록할 수 있게 해준다. 일반적인 OS는 app/OS component가 dynamic cache를 가지더라도, 그 크기를 미리 정해놓는다. 시스템에서 가용한 메모리보다 많이 사용하지 않고, 다른 component들에 영향도 주지 않되 available memory를 잘 이용하고 싶은 것이다. 메모리를 아주 많이 쓰며 한 component에서 다른 component로 메모리가 flow하는 경우 이 문제는 더욱 복잡해진다. **Shrinker API는 app/OS component가 memory pressure 생기면 (커널이 미리 관리할 필요 없이) 직접 해결하도록 한다.**

Shrinker API가 유용한 예시
1. Memcached : 사전에 in-memory cache size를 지정해야 하지만, shrinker API를 통해 OSv가 필요로 하지 않는 모든 메모리를 캐시로 사용
1. JVM : 사전에 maximum heap size를 정해주어야 하지만, shrinker API를 통해 heap size를 자동으로 조절하되 unmodified JVM과 똑같이 동작함 -> section 3.2

### 3.2 JVM Balloon

*JVM balloon*은 JVM heap size를 자동으로 정하되, unmodified JVM과 똑같이 동작하며 모든 hypervisor에서 동작한다.  
*JVM balloon*의 목표는 충분한 dynamic memory placement를 제공하며, 사전에 계획해야할 일을 줄이는 것이다.  
JVM code를 수정하는 대신 OS에서 (JVM balloon을?) 실행하면, 다양한 JVM의 구현/버전을 신경 쓸 필요가 없다.  

JVM heap이 크면 GC cycle의 발생 횟수가 줄어 성능이 향상되지만 (이해 못 함), OS에게 주어진 메모리가 적어질 수 있다. 일반적인 OS는 heap의 page들의 내용을 모르고 page를 evict함으로써 disk activity와 suboptimal cache growth가 발생한다. OS는 어느 페이지가 차후에 필요한지 몰라서 evicting을 자제하게 되는데, 사실 evicting이 성능 향상에 중요한 것이다. OSv는 disk activity 없이 heap page를 버릴 수 있음을 보이겠다.

OSv에서는 우선 JVM에 거의 모든 메모리를 준뒤, OS에서 memory pressure가 생기면 그 때 JNI를 이용하여 heap에서 memory pressure를 해결할 크기의 object(ByteArray)를 포인터 형태로 받는다. 이 object는 JNI에 의해 참조되어서 GC가 free하지 않고, 이 때 OS가 필요한 만큼만 object에 배정해서 heap size도 그만큼만 줄어든다.  
(위와 같은 문단이지만 맥락이 이해 안 되는 내용) ballon은 실제 페이지를 backing storage로 갖고 있기 때문에, unmapping함으로써 이를 OS에 돌려주며 ballooning process가 끝난다. 일부 메모리는 자바 app에서도 사용되지 않고 OS로도 반환되지 않아서, 이를 해결하고자 충분히 큰 minimum balloon size (128MB)를 이용한다.

#### Balloon movement

읽는 중